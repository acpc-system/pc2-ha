URL:
https://linbit.com/drbd-user-guide/drbd-guide-9_0-en/#p-build-install-configure

MySQL, DRBD, and pacemaker 
https://wiki.myhypervisor.ca/books/linux/page/drbd-pacemaker-corosync-mysql-cluster-centos7


Gamed:
https://houseofbrick.com/blog/active-passive-cluster-for-near-ha-using-pacemaker-drbd-corosync-and-mysql/
Gamed: Syntax of new pcs
https://clusterlabs.org/pacemaker/doc/deprecated/en-US/Pacemaker/2.0/html-single/Clusters_from_Scratch/index.html
https://clusterlabs.org/pacemaker/doc/2.1/Pacemaker_Explained/html/advanced-options.html
Gamed:
http://www.linux-ha.org/doc/dev-guides/ra-dev-guide.html

Case setup:
5:Networking
	PC2Master nic1 (NAT net): normal traffic : 10.0.10.4
		  nic2 (vboxnet0): DRBD traffic : 10.1.11.4
		  nic3 (vboxnet1): Cluster traffic: 10.1.12.4
	PC2Slave nice1 (NAT net): normal traffic : 10.0.10.5
                  nic2 (vboxnet0): DRBD traffic : 10.1.11.5
                  nic3 (vboxnet1): Cluster traffic: 10.1.12.5

10-Install DRBD:
	1-Add linbit to launchpad
	sudo apt-add-repository ppa:linbit/linbit-drbd9-stack
	2-Update 
	sudo apt update
	3-Install DRBD Packages
	apt install unzip drbd-utils drbd-dkms default-jre pacemaker corosync pcs wget
15:Create logical volumes: on all nodes
	10-Create partitions on the disk
		fdisk /dev/sdb
	20-Create PV on /dev/sdb1
		sudo pvcreate /dev/sdb1
	30-Create VG contains created PV from step 20
		sudo vgcreate pc2datavg /dev/sdb1
	40-Create a LV occupies all VG space
		sudo lvcreate --name pc2datalv -l 100%FREE pc2datavg
	
20-Configure DRBD:
	10-Configure /etc/drbd.d/global_common.conf: all nodes
		in "common" section, locate "net" and add to it
		protocol C;
	20-Configure resources file, : all nodes
		in /etc/drbd.d/pc2data.res
		resource "pc2data" {
  			device minor 1;
  			disk "/dev/pc2datavg/pc2datalv";
  			meta-disk internal;

  			on "pc2master" {
    				Inode-id 0;
  			}
  			on "pc2slave" {
    				node-id 1;
  			}
  			connection {
    				host "pc2master" address 10.0.12.4:7789;
    				host "pc2slave" address 10.0.12.5:7789;
  			}
		}

	30-Initialize the resource:  all nodes
		sudo drbdadm create-md pc2data
	40-Bringe resource up: all nodes
		sudo sudo drbdadm up pc2data
		cat /proc/drbd

	50-Enable the master node
		sudo drbdadm primary --force pc2data
		cat /proc/drbd
	60-Format the partition:  master node
		mkfs â€“t xfs /dev/drbd0
	70-Check for the resource status
		sudo drbdadm status pc2data
	80-To get cluster state
		sudo drbdadm cstate pc2data
	90-To get node role
		sudo drbdadm role pc2data
	100-To get disk state
		sudo drbdadm dstate pc2data
	110-Mount the drbd device : all
		sudo mkdir /pc2data
		sudo mount /dev/drbd0 /pc2data

30-Configure the cluster
	20-Enable all services: all nodes
		sudo systemctl enable pacemaker corosync pcsd
	30-Set password for hacluster ; all nodes
		sudo passwd hacluster
	40-Start pcsd : all nodes
		sudo systemctl start pcsd
	50-Authenticate the cluster managers: master node
		sudo pcs host auth hbam hbas
	60-Create the cluster: master node
		sudo pcs cluster setup  pc2cluster hbam hbas --force
	70-start pacemaker: all modes
		sudo systemctl start pacemaker
	80-Disable stonith
		sudo pcs property set stonith-enabled=FALSE
	90-Start all services: all nodes
		sudo pcs cluster start
		sudo systemctl start corosync.service
	100-Check cluster state/config: all nodes
		sudo pcs status
		sudo pcs config
	110-Check cluster members
		sudo corosync-cmapctl | grep members
	120-get cluster propert : all nodes
		sudo pcs  property
	130-Get existing configuration: all nods
		sudo pcs cluster cib
	140-Validate configuration 
		sudo crm_verify -L -V
40-Creating IP resources
	150-Create VIP resource: master node
		sudo pcs resource create ClusterVIP ocf:heartbeat:IPaddr2 ip=10.0.10.3 cidr_netmask=24 op monitor interval=30s
	160-TestIP Fail over
		sudo pcs cluster stop pc2master
		and check on the other node	
			sudo pcs cluster status
			ip a

50-Create PC2 service resource
** to list ocf:heartbeat:
	pcs resource agents ocf:heartbeat


	10-Create PC2service : master node
		sudo pcs resource create PC2Service ocf:heartbeat:PC2 extraargs="--nogui --login s1 --contestpassword contest" user="pc2" pidfile="/var/run/pc2.pid" workdir="/pc2data" conffile="/usr/local/pc2/pc2v9.ini" op monitor  OCF_CHECK_LEVEL="0" timeout="20s" interval="10s"

	20-Adjust the timeout
		sudo pcs resource op defaults timeout=240s

	30-Force pacemake to start the PC2Service on the same host with IP: master
		sudo pcs constraint colocation add PC2Service with ClusterVIP INFINITY

	40-Force start in order
		sudo pcs constraint order ClusterVIP then PC2Service

60-Create DRBD service resource
	10-Save the cib config into file: master node
		sudo pcs cluster cib drbd_cfg
	20-Create the raw service: master node
		sudo pcs -f drbd_cfg resource create pc2dataraw ocf:linbit:drbd  drbd_resource=pc2data op monitor interval=60s
	30-Create master service: master node
		sudo pcs -f drbd_cfg resource promotable pc2dataraw promoted-max=1 promoted-node-max=1 clone-max=2 clone-node-max=1 notify=true
	40-Commit changes form file to cib: master node
		sudo pcs cluster cib-push drbd_cfg --config

70-Create FS mounts resource
	10-Save the cib config to file: master node
		sudo pcs cluster cib fs_cfg	
	20-Create raw service : master node
		sudo pcs -f fs_cfg resource create pc2dataFS Filesystem device="/dev/drbd1" directory="/pc2data" fstype="xfs"
	30-Create colocation constraint with DRBD service: master node
		sudo pcs -f fs_cfg constraint colocation add pc2dataFS with pc2dataraw-clone INFINITY with-rsc-role=Master
	40-Configure order constraint: master node
		 sudo pcs -f fs_cfg constraint order promote pc2dataraw-clone then start pc2dataFS

	50-Configure colocation between PC2Service and pc2dataFS: master node
		sudo pcs -f fs_cfg constraint colocation add PC2Service with pc2dataFS INFINITY
	60-Configure order of start: master node
		sudo pcs -f fs_cfg constraint order pc2dataFS then PC2Service
	70-Update the CIB configuration: master node
		sudo pcs -f fs-cfg constraint
		sudo pcs cluster cib-push fs_cfg --config


------------
To query standby status
sudo crm_standby -G --node hbas
To change state of standby
sudo crm_standby -v off
